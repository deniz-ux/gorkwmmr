{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8ba753f-bd74-4b76-9228-abd1fdffbe69",
   "metadata": {},
   "source": [
    ":::{note} Tutorial 8. **Scaling**\n",
    ":icon: false\n",
    "\n",
    "#### Async, threading, profiling and caching\n",
    "\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45577ae0-b570-48f0-8187-17d6090a341e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import panel as pn\n",
    "\n",
    "pn.extension('tabulator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9881f71-cc41-48d9-aed8-ed3a20b1c2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.config.authorize_callback = authorized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074c4205-2dbd-4956-b01c-be67ad89c703",
   "metadata": {},
   "source": [
    "## Async"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32069457-6c3d-4822-b3e8-18b4ef5dd02d",
   "metadata": {},
   "source": [
    "Let us say we have some action that is triggered by a widget, such as a button, and while we are computing the results we want to provide feedback to the user. Using imperative programming this involves writing callbacks that update the current state of our components. This is complex and really we prefer to write reactive components. This is where *generator functions* come in.\n",
    "\n",
    ":::{important}\n",
    "A *generator* function is a function that use `yield` to *return* results as they are produced during the execution. It is not allowed to `return` anything, but can use `return` to *break* the execution. For an introduction to *generator functions* check out [Real Python | Introduction to generator functions](https://realpython.com/introduction-to-python-generators/).\n",
    ":::\n",
    "\n",
    "In the example below we add a `Button` to trigger some calculation. Initially the calculation hasn't yet run, so we check the value provided by the `Button` indicating whether a calculation has been triggered and while it is `False` we `yield` some text and `return`. However, when the `Button` is clicked the function is called again with `run=True` and we kick off some calculation. As this calculation progresses we can `yield` updates and then once the calculation is successful we `yield` again with the final result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb06e55-33c6-44e9-8a31-cde6c61d51dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = pn.widgets.Button(name=\"Press to run calculation\", align='center')\n",
    "\n",
    "def runner(run):\n",
    "    if not run:\n",
    "        yield \"Calculation did not run yet\"\n",
    "        return\n",
    "    for i in range(101):\n",
    "        time.sleep(0.01) # Some calculation\n",
    "        yield pn.Column(\n",
    "            f'Running ({i}/100%)', pn.indicators.Progress(value=i)\n",
    "        )\n",
    "    yield \"Success ✅︎\"\n",
    "\n",
    "pn.Row(run, pn.bind(runner, run))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda9a1ec-307c-4984-ba5f-a64c07da5fa7",
   "metadata": {},
   "source": [
    "This provides a powerful mechanism for providing incrememental updates as we load some data, perform some data processing, etc.\n",
    "\n",
    "This can also be combined with asynchronous processing, e.g. to dynamically stream in new data as it arrives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a310502f-2006-4aab-b3ad-84179b1e7745",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "async def slideshow():\n",
    "    index = 0\n",
    "    while True:\n",
    "        url = f\"https://picsum.photos/800/300?image={index}\"\n",
    "        \n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            async with session.get(url) as resp:\n",
    "                img, _ = await asyncio.gather(resp.read(), asyncio.sleep(1))\n",
    "                yield pn.pane.JPG(img)\n",
    "        index = (index + 1) % 10\n",
    "\n",
    "pn.Row(slideshow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3705dd46-ba07-49ba-9833-58b8df676657",
   "metadata": {},
   "source": [
    "## Enable Automatic Threading\n",
    "\n",
    "Using threading in Panel can either be enabled manually, e.g. by managing your own thread pool and dispatching concurrent tasks to it, or it can be managed by Panel itself by setting the `config.nthreads` parameter (or equivalently by setting it with `pn.extension(nthreads=...)`. This will start a `ThreadPoolExecutor` with the specified number of threads (or if set to `0` it will set the number of threads based on your system, i.e. `min(32, os.cpu_count() + 4)`).\n",
    "\n",
    "Whenever an event is generated or a periodic callback fires Panel will then automatically dispatch the event to the executor. An event in this case refers to any action generated on the frontend such as the manipulation of a widget by a user or the interaction with a plot. If you are launching an application with `panel serve` you should enable this option configure this option on the CLI by setting `--num-threads`.\n",
    "\n",
    "To demonstrate the effect of enabling threading take this example below:\n",
    "\n",
    "```python\n",
    "import panel as pn\n",
    "\n",
    "pn.extension(nthreads=2)\n",
    "\n",
    "def button_click(event):\n",
    "    print(f'Button clicked for the {event.new}th time.')\n",
    "    time.sleep(2) # Simulate long running operation\n",
    "    print(f'Finished processing {event.new}th click.')\n",
    "\n",
    "button = pn.widgets.Button(name='Click me!')\n",
    "\n",
    "button.on_click(button_click)\n",
    "```\n",
    "\n",
    "When we click the button twice successively in a single-threaded context we will see the following output:\n",
    "\n",
    "```\n",
    "> Button clicked for the 1th time.\n",
    "... 2 second wait\n",
    "> Finished processing 1th click.\n",
    "> Button clicked for the 2th time.\n",
    "... 2 second wait\n",
    "> Finished processing 2th click.\n",
    "```\n",
    "\n",
    "In a threaded context on the other hand the two clicks will be processed concurrently:\n",
    "\n",
    "```\n",
    "> Button clicked for the 1th time.\n",
    "> Button clicked for the 2th time.\n",
    "... 2 second wait\n",
    "> Finished processing 1th click.\n",
    "> Finished processing 2th click.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1102890b-a580-4ef7-a8f9-1921d78489d6",
   "metadata": {},
   "source": [
    "## Launch profiling\n",
    "\n",
    "The launch profiler profiles the execution time of the initialization of a particular application. It can be enabled by setting a profiler using the commandline ``--profiler`` option to `panel serve`. Available profilers include:\n",
    "\n",
    "- [`pyinstrument`](https://pyinstrument.readthedocs.io): A statistical profiler with nice visual output\n",
    "- [`snakeviz`](https://jiffyclub.github.io/snakeviz/): SnakeViz is a browser based graphical viewer for the output of Python’s cProfile module and an alternative to using the standard library pstats module.\n",
    "- [`memray`](https://bloomberg.github.io/memray/): memray is a memory profiler for Python. It can track memory allocations in Python code, in native extension modules, and in the Python interpreter itself.\n",
    "\n",
    "Once enabled the launch profiler will profile each application separately and provide the profiler output generated by the selected profiling engine.\n",
    "\n",
    ":::{image} ./assets/launch_profiler.png\n",
    ":width: 80%\n",
    ":::\n",
    "\n",
    "## User profiling\n",
    "\n",
    "In addition to profiling the launch step of an application it is often also important to get insight into the interactive performance of an application. For that reason Panel also provides the `pn.io.profile` decorator that can be added to any callback and will report the profiling results in the `/admin` panel. The `profile` helper takes to arguments, the name to record the profiling results under and the profiling `engine` to use.\n",
    "\n",
    "```python\n",
    "@pn.io.profile('clustering', engine='snakeviz')\n",
    "def get_clustering(event):\n",
    "    # some expensive calculation\n",
    "    ...\n",
    "\n",
    "widget.param.watch(my_callback, 'value')\n",
    "```\n",
    "\n",
    ":::{image} ./assets/user_profiling.png\n",
    ":width: 80%\n",
    ":::\n",
    "\n",
    "The user profiling may also be used in an interactive session, e.g. we might decorate a simple callback with the `profile` decorator:\n",
    "\n",
    "```python\n",
    "import time\n",
    "\n",
    "slider = pn.widgets.FloatSlider(name='Test')\n",
    "\n",
    "@pn.io.profile('formatting')\n",
    "def format_value(value):\n",
    "    time.sleep(1)\n",
    "    return f'Value: {value+1}'\n",
    "\n",
    "pn.Row(slider, pn.bind(format_value, slider))\n",
    "```\n",
    "\n",
    "Then we can request the named profile 'formatting' using the `pn.state.get_profile` function:\n",
    "\n",
    "```python\n",
    "pn.state.get_profile('formatting')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5228dcf0-128f-4ec4-bdb6-efbd74f41134",
   "metadata": {},
   "source": [
    "## Caching\n",
    "\n",
    "The `panel.state.cache` object is a simple dictionary that is shared between all sessions on a particular Panel server process. This makes it possible to load large datasets (or other objects you want to share) once and subsequently access the cached object.\n",
    "\n",
    "To assign to the cache manually, simply put the data load or expensive calculation in an `if`/`else` block which checks whether the custom key is already present:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe79128-1e51-4193-964f-41e307108ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'data' in pn.state.cache:\n",
    "    data = pn.state.cache['data']\n",
    "else:\n",
    "    pn.state.cache['data'] = data = pd.read_parquet('./windturbines.parq')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafe10dd-3246-45fe-9957-da0f241fba40",
   "metadata": {},
   "source": [
    "Alternatively, the `as_cached` helper function provides a slightly cleaner way to write the caching logic. Instead of writing a conditional statement you write a function that is executed only when the inputs to the function change. If provided, the `args` and `kwargs` will also be hashed making it easy to cache (or memoize) on the arguments to the function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43030cd-3eea-4fb2-92ce-fcdc935ce6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pn.state.as_cached('data', pd.read_parquet, path='./windturbines.parq', ttl=1200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88569f9a-e8d2-450b-a68d-1868a8150a41",
   "metadata": {},
   "source": [
    "Now, the first time the app is loaded the data will be cached and subsequent sessions will simply look up the data in the cache, speeding up the process of rendering. If you want to warm up the cache before the first user visits the application you can also provide the `--warm` argument to the `panel serve` command, which will ensure the application is initialized as soon as it is launched. If you want to populate the cache in a separate script from your main application you may also provide the path to a setup script using the `--setup` argument to `panel serve`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580e3585-05b7-47ea-9ab8-7aab0b89eaf1",
   "metadata": {},
   "source": [
    "## Memoization\n",
    "\n",
    "The `pn.cache` decorator provides an easy way to cache the outputs of a function depending on its inputs (i.e. `memoization`). If you've ever used the Python `@lru_cache` decorator you will be familiar with this concept. However the `pn.cache` functions support additional cache `policy`s apart from LRU (least-recently used), including `LFU` (least-frequently-used) and 'FIFO' (first-in-first-out). This means that if the specified number of `max_items` is reached Panel will automatically evict items from the cache based on this `policy`. Additionally items can be deleted from the cache based on a `ttl` (time-to-live) value given in seconds.\n",
    "\n",
    "### Caching in memory\n",
    "\n",
    "The `pn.cache` decorator can easily be combined with the different Panel APIs including `pn.bind` and `pn.depends` providing a powerful way to speed up your applications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dccd2bd-15b7-4a6c-8520-5e8dd1def0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pn.cache(max_items=10, policy='LFU')\n",
    "def load_data(path):\n",
    "    return ... # Load some data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccf132e-a210-4611-876e-6f81baeab0e5",
   "metadata": {},
   "source": [
    "Once you have decorated your function with `pn.cache` any call to `load_data` will be cached in memory until `max_items` value is reached (i.e. you have loaded 10 different `path` values). At that point the `policy` will determine which item is evicted.\n",
    "\n",
    "The `pn.cache` decorator can easily be combined with `pn.bind` to speed up rendering of your reactive components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8da728d-5be4-4458-9d10-f2b488a667e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import panel as pn\n",
    "\n",
    "select = pn.widgets.Select(options={\n",
    "    'Penguins': 'https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv',\n",
    "    'Diamonds': 'https://raw.githucbusercontent.com/mwaskom/seaborn-data/master/diamonds.csv',\n",
    "    'Titanic': 'https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv',\n",
    "    'MPG': 'https://raw.githubusercontent.com/mwaskom/seaborn-data/mastser/mpg.csv'\n",
    "})\n",
    "\n",
    "@pn.cache\n",
    "def fetch_data(url):\n",
    "    return pd.read_csv(url)\n",
    "\n",
    "pn.Column(select, pn.widgets.Tabulator(pn.bind(fetch_data, select), page_size=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4a9cb2-a47d-4311-baaa-ddc3fb46cd07",
   "metadata": {},
   "source": [
    "\n",
    "### Disk caching\n",
    "\n",
    "If you have `diskcache` installed you can also cache the results to disk by setting `to_disk=True`. The `diskcache` library will then cache the value to the supplied `cache_path` (defaulting to `./cache`). Making use of disk caching allows you to cache items even if the server is restarted.\n",
    "\n",
    "### Clearing the cache\n",
    "\n",
    "Once a function has been decorated with `pn.cache` you can easily clear the cache by calling `.clear()` on that function, e.g. in the example above you could call `load_data.clear()`. If you want to clear all caches you may also call `pn.state.clear_caches()`.\n",
    "\n",
    "### Per-session caching\n",
    "\n",
    "By default any functions decorated or wrapped with `pn.cache` will use a global cache that will be reused across multiple sessions, i.e. multiple users visiting your app will all share the same cache. If instead you want a session-local cache, that only reuses cached outputs for the duration of each visit to your application, you can set `pn.cache(..., per_session=True)`.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
