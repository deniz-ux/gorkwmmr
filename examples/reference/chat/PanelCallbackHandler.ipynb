{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Langchain `PanelCallbackHandler` is useful for rendering and streaming *chain of thought* from Langchain objects like Tools, Agents, and Chains. It inherits from Langchain's [BaseCallbackHandler](https://python.langchain.com/docs/modules/callbacks/).\n",
    "\n",
    "#### Parameters:\n",
    "\n",
    "##### Core\n",
    "\n",
    "* **`instance`** (ChatFeed | ChatInterface): The ChatFeed or ChatInterface instance to render or stream to.\n",
    "* **`user`** (str): Name of the user who sent the message.\n",
    "* **`avatar`** (str | BinaryIO): The avatar to use for the user. Can be a single character text, an emoji, or anything supported by `pn.pane.Image`. If not set, uses the first character of the name.\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basics\n",
    "\n",
    "To get started:\n",
    "\n",
    "1. Pass the instance of a `ChatFeed` or `ChatInterface` to `PanelCallbackHandler`.\n",
    "2. Pass the `callback_handler` as a list into `callbacks` when constructing or using Langchain objects.\n",
    "\n",
    "\n",
    "```python\n",
    "import panel as pn\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "pn.extension()\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "def callback(contents, user, instance):\n",
    "    callback_handler = pn.chat.langchain.PanelCallbackHandler(instance)    \n",
    "    llm.predict(contents, callbacks=[callback_handler])\n",
    "\n",
    "pn.chat.ChatInterface(callback=callback).servable()\n",
    "```\n",
    "\n",
    "![Panel Callback Handler Basic](../../assets/panel_callback_handler_basic.png)\n",
    "\n",
    "This example shows the response from the `llm` only. A `llm` by it self does not show any *chain of thought*. Later we will build an agent that uses tools. This will show *chain of thought*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Async\n",
    "\n",
    "`async` can also be used. This will make your app more *responsive* and enable it to support more users.\n",
    "\n",
    "```python\n",
    "import panel as pn\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "pn.extension()\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "async def callback(contents, user, instance):\n",
    "    callback_handler = pn.chat.langchain.PanelCallbackHandler(instance)\n",
    "    await llm.apredict(contents, callbacks=[callback_handler])\n",
    "\n",
    "pn.chat.ChatInterface(callback=callback).servable()\n",
    "```\n",
    "\n",
    "![Panel Callback Handler Basic](../../assets/panel_callback_handler_basic.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Streaming\n",
    "\n",
    "To stream tokens from the LLM, simply set `streaming=True` when constructing the LLM.\n",
    "\n",
    "Note, `async` is not required to stream, but it is more efficient.\n",
    "\n",
    "```python\n",
    "import panel as pn\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "pn.extension()\n",
    "\n",
    "llm = OpenAI(temperature=0, streaming=True)\n",
    "\n",
    "async def callback(contents, user, instance):\n",
    "    callback_handler = pn.chat.langchain.PanelCallbackHandler(instance)\n",
    "    await llm.apredict(contents, callbacks=[callback_handler])\n",
    "\n",
    "pn.chat.ChatInterface(callback=callback).servable()\n",
    "```\n",
    "\n",
    "<video controls=\"\" poster=\"../../assets/panel_callback_handler_basic.png\">\n",
    "    <source src=\"https://user-images.githubusercontent.com/42288570/276449298-b8c774a9-d4f3-4388-93c3-01c901f3cd4c.mp4\" type=\"video/mp4\" style=\"max-height: 400px; max-width: 600px;\">\n",
    "    Your browser does not support the video tag.\n",
    "</video>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agents & Tools\n",
    "\n",
    "To differentiate between agents', tools', and other LLM output (i.e. automatically use corresponding avatars and names), be sure to also provide `callback_handler` to their constructors.\n",
    "\n",
    "Again, `async` is not required, but more efficient.\n",
    "\n",
    "```python\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.agents import AgentType, initialize_agent, load_tools\n",
    "import panel as pn\n",
    "\n",
    "llm = OpenAI(temperature=0, streaming=True)\n",
    "tools = load_tools([\"ddg-search\"])\n",
    "agent = initialize_agent(\n",
    "    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True, \n",
    ")\n",
    "\n",
    "async def callback(contents, user, instance):\n",
    "    callback_handler = pn.chat.langchain.PanelCallbackHandler(instance)\n",
    "    await agent.arun(contents, callbacks=[callback_handler])\n",
    "\n",
    "pn.chat.ChatInterface(callback=callback).servable()\n",
    "```\n",
    "\n",
    "![PanelCallbackHandler Agent](../../assets/panel_callback_handler_agent.png)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
